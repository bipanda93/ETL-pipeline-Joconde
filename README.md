# üß¨ Classification ML ‚Äî SVM vs KNN
### Breast Cancer Wisconsin Dataset

> Projet de machine learning supervis√© comparant deux algorithmes de classification binaire pour la d√©tection de tumeurs malignes/b√©nignes.

**Auteur :** Bipanda Franck Ulrich  
**Environnement :** PyCharm | Python 3 | scikit-learn  
**Date :** F√©vrier 2026

---

## üìã Description du Projet

Ce projet impl√©mente et compare deux algorithmes de classification supervis√©e :

- **SVM** (Support Vector Machine) ‚Äî `script1.py`
- **KNN** (K-Nearest Neighbors) ‚Äî `script2.py`

L'objectif est de pr√©dire si une tumeur est **maligne (0)** ou **b√©nigne (1)** √† partir de 30 features cliniques mesur√©es par imagerie m√©dicale.

---

## üì¶ Installation

### Pr√©requis
- Python 3.8+
- PyCharm (recommand√©) ou tout autre IDE Python

### Installer les d√©pendances

```bash
pip install scikit-learn numpy pandas matplotlib seaborn
```

### Cloner le projet

```bash
git clone https://github.com/bipanda-franck-ulrich/classification-ml.git
cd classification-ml
```

---

## üóÇÔ∏è Structure du Projet

```
classification-ml/
‚îÇ
‚îú‚îÄ‚îÄ script1.py       # Mod√®le SVM
‚îú‚îÄ‚îÄ script2.py       # Mod√®le KNN
‚îî‚îÄ‚îÄ README.md        # Documentation
```

---

## üß™ Dataset

| Caract√©ristique | Valeur |
|----------------|--------|
| Nom | Breast Cancer Wisconsin |
| Source | `sklearn.datasets.load_breast_cancer()` |
| √âchantillons | 569 |
| Features | 30 |
| Classes | malignant (0) / benign (1) |
| Split Train/Test | 80% / 20% |

---

## üíª Codes Sources

### Script 1 ‚Äî SVM (`script1.py`)

```python
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import load_breast_cancer

breast_cancer = load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy :", accuracy_score(Y_test, Y_pred))
print(confusion_matrix(Y_test, Y_pred))
print(classification_report(Y_test, Y_pred))

# Test de plusieurs valeurs de C
for c in [0.1, 1, 10, 100]:
    model = SVC(C=c, kernel='rbf')
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("C =", c, "-> Accuracy =", accuracy_score(Y_test, y_pred))
```

---

### Script 2 ‚Äî KNN (`script2.py`)

```python
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import load_breast_cancer

breast_cancer = load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = KNeighborsClassifier()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy :", accuracy_score(Y_test, Y_pred))
print(confusion_matrix(Y_test, Y_pred))
print(classification_report(Y_test, Y_pred))

# Test de plusieurs valeurs de K
for k in [1, 3, 5, 7, 9]:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("K =", k, "-> Accuracy =", accuracy_score(Y_test, y_pred))
```

---

## üìä R√©sultats et M√©triques

### Accuracy Globale

| Mod√®le | Meilleur param√®tre | Accuracy |
|--------|-------------------|----------|
| **SVM** | C = 1 | **98.25%** ‚úÖ |
| KNN | K = 9 | 96.49% |

---

### Matrice de Confusion

**SVM (C=1, avec StandardScaler) :**

|  | Pr√©dit Malin (0) | Pr√©dit B√©nin (1) |
|--|-----------------|-----------------|
| **R√©el Malin (0)** | 41 ‚úÖ | 2 ‚ùå |
| **R√©el B√©nin (1)** | 0 ‚úÖ | 71 ‚úÖ |

**KNN (K=5, avec StandardScaler) :**

|  | Pr√©dit Malin (0) | Pr√©dit B√©nin (1) |
|--|-----------------|-----------------|
| **R√©el Malin (0)** | 40 ‚úÖ | 3 ‚ùå |
| **R√©el B√©nin (1)** | 3 ‚ùå | 68 ‚úÖ |

---

### Rapport de Classification ‚Äî SVM

| Classe | Pr√©cision | Rappel | F1-Score |
|--------|-----------|--------|----------|
| malignant (0) | 1.00 | 0.95 | 0.98 |
| benign (1) | 0.97 | 1.00 | 0.99 |
| **accuracy** | | | **0.98** |

---

### Optimisation des Hyperparam√®tres

**SVM ‚Äî Valeurs de C :**

| C | Accuracy |
|---|----------|
| 0.1 | 94.74% |
| **1** | **98.25% ‚òÖ** |
| 10 | 97.37% |
| 100 | 93.86% |

**KNN ‚Äî Valeurs de K :**

| K | Accuracy |
|---|----------|
| 1 | 93.86% |
| 3 | 94.74% |
| 5 | 94.74% |
| 7 | 94.74% |
| **9** | **96.49% ‚òÖ** |

---

## üèÜ Conclusion

Le **SVM avec C=1 et StandardScaler** est l'algorithme recommand√© pour cette t√¢che :

- ‚úÖ Accuracy de **98.25%** vs 96.49% pour KNN
- ‚úÖ **0 faux positif** (aucun patient sain diagnostiqu√© malin √† tort)
- ‚úÖ Seulement **2 faux n√©gatifs** vs 3 pour KNN
- ‚úÖ Meilleur rappel sur la classe maligne : **95%** vs 93%

> Dans un contexte m√©dical, minimiser les faux n√©gatifs est critique car cela repr√©sente des tumeurs malignes non d√©tect√©es.

---

## üîß Technologies

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-latest-F7931E?style=flat&logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-latest-150458?style=flat&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-latest-013243?style=flat&logo=numpy&logoColor=white)
